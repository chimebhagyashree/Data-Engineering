# PROJECT : DATA LAKE

## Introduction

A music streaming startup, Sparkify, has grown their user base and song database even more and want to move their data warehouse to a data lake. Data Lake is a new form of data warehouse that evolved to cope with variety of data formats and structuring, machine generated data, wide spectrum data transformation needed by advanced analytics. Their data resides in AWS S3, in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.

Being their data engineer, my responsibility is to build an ETL pipeline that extracts their data from AWS S3, processes them using Spark, do data wrangling and data quality checks and loads the data back into S3 as a set of dimensional tables in parquet file format. This will allow their analytics team to continue finding insights in what songs their users are listening to.


## Project Description

In this project,I have tried to apply what I have learned on Spark,data wrangling with Spark,Data Lake with Spark and setting up spark clusters AWS EMR to build an ETL pipeline for data lake hosted on AWS S3.


 As per the scope of the project, I have created EMR spark cluster and using spark session I have loaded the song and user log data from S3, done the data transformation as per the business logic using Spark and landed them back into S3 bucket in parquet file format.  To define stage and target tables for song and events data, I have written an ETL pipeline that extracts data from files in two directories in S3  and processed them by creating Spark dataframes for a particular analytic focus like the users and the type of songs they listen to ,extracted time information to create time dimension table and loaded target tables data back to S3 in parquet file format. AWS S3 provides consistent and reliable cloud storage service to store this data for further analysis. Parquet file allows organizing data in columnar storage format for flexible compression and faster read performace.
 
 
## Purpose

- To understand and analyse the data resides in AWS S3 in directories of song and events in JSON format
- To create an EC2 instance on which EMR cluster will be created.
- To create EMR cluster on which Notebook for spark kernel will be created.
- To create Jupyter Notebook using the active EMR cluster.
- To create Spark session on Notebook.
- To extarct song and user related **JSON** data from S3 by creating spark dataframes.
- To perform data wrangling on extracted data and process them.
- To do unit testing on the target dataframs in order to match the expectation of analytics team.
- To populate data in the target tables in parquet file format in S3.


## Project Datasets

I am provided with two datasets that reside in AWS S3. Here are the S3 links for each:

- Song data: <s3://udacity-dend/song_data>
- Log data: <s3://udacity-dend/log_data>

### Song Dataset

The song files are in S3 bucket in JSON format and contains metadata about a song and the artist of that song.The files are partitioned by the first three letters of each song's track ID.The files are partitioned by the first three letters of each song's track ID. Below are some examples of filepaths to song files in the song dataset.

> `song_data/A/B/C/TRABCEI128F424C983.json`
> `song_data/A/A/B/TRAABJL12903CDCF1A.json`


### Log Dataset

The second dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate app activity logs from an music streaming app based on configuration settings.These log files are partitioned by year and month.Below are some examples of filepaths to log files in the log dataset.


>  `log_data/2018/11/2018-11-12-events.json`
>  `log_data/2018/11/2018-11-13-events.json`



## Schema for Song Play Analysis

- For this project, a star schema model has been optimized to query on song play analysis using the song and log datasets.
- This design includes one fact table and four dimension tables  to store the song and event log dataset in an organized manner for song play analysis.
  
- Following are the table names and their definations.

###  Fact Table

- **songplays** - records in log data associated with song plays i.e. records with page **NextSong**
  Fields include : **songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent, month,year**
  
- *month* and *year* fields have been added in this file for storing the songplays files in S3 based on `year` and `month` partition.

### Dimension Tables

1. **users** - consolidates information about the users in the music streaming app
   Fields include : **user_id, first_name, last_name, gender, level**

2. **songs** - captures songs data in music database
   Fields include : **song_id, title, artist_id, year, duration**

3. **artists** - records information about artists of corresponding song in music database
   Fields include : **artist_id, name, location, lattitude, longitude**

4. **time** - timestamps of records in songplays broken down into specific units
   Fields include :**start_time, hour, day, week, month, year, weekday**
   
- songs table file has been partitioned by `year` and `artists_id` while loaded into S3. Similarly, time table has been partitioned by `year` and `month`. 
- Duplicates and NULL values in files have been handled by using spark `dropDuplicates()`, `isNotNull()` functions.

- As part of this project, ETL pipeline has been designed using pyspark and python with different libraries like configparser,pyspark,os,datetime which helps in authorizing connection to AWS S3 and processing the data and writing back to S3 output.

## ETL Design

- CONFIG FILE : As part of this project, **dl.cfg** config file has been created to encapsulate credentials for AWS services connection. This file basically holds AWS ACCESS KEY and AWS SECRET ACCESS KEY information.

- BUILD ETL Pipeline : **etl.py** file has been created which contains the ETL spark coding for each table.It reads data from S3, processes that data using Spark, and writes them back to S3.Here, two spark functions `process_song_data` and `process_log_data` have been defined to facilitate the ETL process. 
    - `process_song_data` function extracts song and corresponding artists information from the song datasets in S3 and process them to load into **songs** and **artists** target table files have been created in S3.
    - `process_log_data` function extracts users information from user log datasets to load data into **users**, extarcts time information from `ts` field from user log dataset and write transformation logic for different fields in **time** table like hour,day,weeek,month,year etc and load into  it and finally extracts users and song information from both sonag and userlog datasets , joins both files based on artist,song and song length  and process them to write into **songplays** table files.
    
## Conclusion

- Based on this project, AWS IAM role,EC2,EMR cluster and Notebook have been created to define the concept of Data Lake. As part of this project, No traditional databases has been created in order to facilitate ETL process like dataware house. ETL process has been carried out on S3 using pyspark to perform analysis on song and user data. The analytics team has got an easy access to query their data and analyze understanding what songs users are listening to.

### Disclaimer

- It shoudl be paid attention to the cluster region, has to be in the same region of S3 bucket in order to work smoothly.For this project, I ahve used **us-west-2** region.
- It is to be ensured to click Restart kernel and refresh workplace to close the connection to the spark session after running the notebook.
- EMR cluster and EC2 instances needs to be terminated after work in order to avoid of extra charges.
- As part of this project, **emr-5.20.0** cluster has been used.
