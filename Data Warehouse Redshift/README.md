# Project : DATA WAREHOUSE IN AWS REDSHIFT

## Introduction

A music streaming startup company called Sparkify, has grown their user base and song database and want to move their data onto the cloud and process there.The analytics team is basically showing their interest in understanding of overall analysis of the music streaming application.Currently, they don't have an easy way to access and query their data, which resides in AWS S3 service in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.

Being an data engineer, my responsibility is to create AWS redshift cluster by assigning proper IAM role, create database schemas, build an ETL pipeline that extracts their data from S3, stages them in Redshift, and transforms data into a set of dimensional tables, do data quality check by running some unit testing queries over target tables to meet the expectations of the analytics team to continue finding insights in what songs their users are listening to.


## Project Description

In this project,I have tried to apply what I have learned on data warehouses and AWS to build an ETL pipeline for a database hosted on Redshift using python.

As per the scope of the project, I have tried to define stage tables for song and events data, Fact and Dimension tables using star schema model for a particular analytic focus like the users and the type of songs they listen to and the artists of those songs and written an ETL pipeline that extracts data from files in two local directories in S3 into stage tables using **COPY** commands and loads data to target tables in Redshift cluster using **SQL**.The Redshift database provides a consistent and reliable cloud source to store this data and further analysis. 

## Purpose

- To understand and analyse the data resides in AWS S3 in directories of song and events in JSON format
- To create an IAM role which allow redshift clusters to call AWS services with attaching a S3 readonly access policy to it.
- To create a redshift cluster where the database can reside.
- To create database schema,tables using STAR schema data model for storing songs and user activity related information.
- To extarct song and user related **JSON** data from S3 to staging tables using **COPY** command.
- To populate data in the target tables from staging through **INSERTS** statements using **SQL**.
- To unit testing on the tables in order to match the expectation of analytics team.

## Project Datasets

I am provided with two datasets that reside in AWS S3. Here are the S3 links for each:

- Song data: <s3://udacity-dend/song_data>
- Log data: <s3://udacity-dend/log_data>
- Log data json path: <s3://udacity-dend/log_json_path.json>


### Song Dataset

The song files are in S3 bucket in JSON format and contains metadata about a song and the artist of that song.The files are partitioned by the first three letters of each song's track ID.The files are partitioned by the first three letters of each song's track ID. Below are some examples of filepaths to song files in the song dataset.


+ `song_data/A/B/C/TRABCEI128F424C983.json`
+ `song_data/A/A/B/TRAABJL12903CDCF1A.json`


### Log Dataset

The second dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate app activity logs from an music streaming app based on configuration settings.These log files are partitioned by year and month.Below are some examples of filepaths to log files in the log dataset.

+ `log_data/2018/11/2018-11-12-events.json`
+ `log_data/2018/11/2018-11-13-events.json`


## Database Schema Design and ETL pipeline

+ For this project,STAR schema model has been used to design the schema in redshift cluster. This design is prefered as it simplifies queries and
  provides fast aggregation of data.
  
+ This design includes one fact table and four dimension tables  to store the song and event log dataset in an organized manner for songplay analysis.
  Following are the tables and their description.
  
###  Fact Table

+ songplays - records in event data associated with song plays i.e. records with page NextSong
   Attributes include : **songplay_id,start_time,user_id,level,song_id,artist_id,session_id,location,user_agent**
 

### Dimension Tables

1. users - consolidates information about the users in the music streaming app
   Attributes include : **user_id, first_name, last_name, gender, level**

2. songs - captures songs data in music database
   Attributes include : **song_id, title, artist_id, year, duration**

3. artists - records information about artists of corresponding song in music database
   Attributes include : **artist_id, name, location, lattitude, longitude**

4. time - timestamps of records in songplays broken down into specific units
   Attributes include :**start_time, hour, day, week, month, year, weekday**
 
- The attribute level constraints like `PRIMARY KEY,NOT NULL,DISTKEY,SORTKEY` have been applied to deal with the relational database system in redshift    cluster. For instance, artist_id, user_id should be considered as PRIMARY KEY in their respective table with NOT NULL constraints. For better query optimization, data are distributed and stored on different distribution style.For example, user_id as DISTKEY and song_id as SORTKEY are considered in songplays fact table.

- For ETL design, python coding has been used as it contains different libraries like configparser,psycopg2 which helps in authorizing connection to different cloud services like AWS S3 and redshift cluster database.

## ETL Design

- CREATE DATABASE AND TABLES: Below steps have been followed to create database and tables.
    1. CREATE config file for cloud services credential authorization and database connection parameter configuration.
    2. CREATE CLUSTER and database using User Interface or infrastructre as code in python.
    3. DROP statements for each tables have been written in **sql_queries.py** to drop tables if already exist.
    4. CREATE statementes for each tables have been written in **sql_queries.py**
    5. RUN **create_tables.py** to create the stage and target tables.
    6. VALIDATE in redshift cluster for table creation.
    7. RUN **etl.py** to extract data from S3 to load into stage table and insert data into target table.
    
- BUILD ETL Pipeline: There is different ETL process for each table.In `etl.py`, there is two functions called `load_staging_tables` and `insert_tables` to maintain ETL.Two query lists *copy_table_queries* and *insert_table_queries* facilitate both songs and log data from S3 to staging tables through `COPY` command and insert into target tables . It extracts all files matching extension(.json) from the directory in S3 and loads into stg_events and stg_songs stage tables and processes using the sql queries to insert into target fact and dimension tables.

- In summary, below two steps need to be accomplished to perform ETL.
1. RUN **create_tables.py** to create the stage and target tables.
2. RUN **etl.py** to populate data in the created tables.


## Conclusion

- As part of this project, AWS IAM role, redshift cluster database have been created to ingest data into stage table from AWS S3 buckets and insert data into target tables from stage table using `SQL` queries to perform analysis on song and user data.The analytics team has got an easy access to query their data and analyze understanding what songs users are listening to.
 
### Disclaimer

- It shoudl be paid attention to the cluster region, has to be in the same region of S3 bucket in order to work smoothly.
- It is to be ensured to click Restart kernel and refresh workplace to close the connection to the database after running the notebook.
- Redshift cluster needs to be deleted after work in order to avoid of surplus charges.

